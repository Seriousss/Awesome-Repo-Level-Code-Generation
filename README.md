<h1 align="center">🤖✨ Awesome Repository-Level Code Generation ✨🤖</h1>

<p align="center">
  <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"></a>
  <img src="https://img.shields.io/github/stars/YerbaPage/Awesome-Repo-Level-Code-Generation?color=yellow&label=⭐ Stars">
  <img src="https://img.shields.io/badge/PRs-Welcome-red">
  <img src="https://img.shields.io/github/last-commit/YerbaPage/Awesome-Repo-Level-Code-Generation?label=⏰ Last%20Commit">
</p>

🌟 A curated list of awesome repository-level code generation research papers and resources. If you want to contribute to this list (please do), feel free to send me a pull request. 🚀

## 📚 Contents

- [📚 Contents](#-contents)
- [💥 Repo-Level Issue Resolution](#-repo-level-issue-resolution)
- [🤖 Repo-Level Code Completion](#-repo-level-code-completion)
- [📊 Datasets and Benchmarks](#-datasets-and-benchmarks)

## 💥 Repo-Level Issue Resolution

- SWE-bench: Can Language Models Resolve Real-World GitHub Issues? [2024-ICLR] [[📄 paper](https://arxiv.org/pdf/2310.06770)] [[🔗 repo](https://github.com/princeton-nlp/SWE-bench)]
  
- How to Understand Whole Software Repository? [2024-arXiv] [[📄 paper](https://arxiv.org/pdf/2406.01422)]

- AutoCodeRover: Autonomous Program Improvement [2024-ISSTA] [[📄 paper](https://dl.acm.org/doi/10.1145/3650212.3680384)] [[🔗 repo](https://github.com/nus-apr/auto-code-rover)

- Lingma SWE-GPT: An Open Development-Process-Centric Language Model for Automated Software Improvement [2024-arXiv] [[📄 paper](https://arxiv.org/html/2411.00622v1)] [[🔗 repo](https://github.com/LingmaTongyi/Lingma-SWE-GPT)]  

- MarsCode Agent: AI-native Automated Bug Fixing [2024-arXiv] [[📄 paper](https://arxiv.org/html/2411.10213v1)]  

- AGENTLESS: Demystifying LLM-based Software Engineering Agents [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2407.01489)]  

- RepoGraph: Enhancing AI Software Engineering with Repository-level Code Graph [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2407.01489)] [[🔗 repo](https://github.com/ozyyshr/RepoGraph)]  

- SWE-Search: Enhancing Software Agents with Monte Carlo Tree Search and Iterative Refinement [2024-arXiv] [[📄 paper](https://arxiv.org/pdf/2410.20285)] [[🔗 repo](https://github.com/aorwall/moatless-tree-search)]

## 🤖 Repo-Level Code Completion

- Fully Autonomous Programming with Large Language Models [2023-GECCO] [[📄 paper](https://dl.acm.org/doi/pdf/10.1145/3583131.3590481)] [[🔗 repo](https://github.com/KoutchemeCharles/aied2023)]

- Repository-Level Prompt Generation for Large Language Models of Code [2023-PMLR] [[📄 paper](https://proceedings.mlr.press/v202/shrivastava23a.html)] [[🔗 repo](https://github.com/shrivastavadisha/repo_level_prompt_generation)]

- RepoFusion: Training Code Models to Understand Your Repository [2023-arXiv] [[📄 paper](https://arxiv.org/abs/2306.10998)] [[🔗 repo](https://github.com/ServiceNow/RepoFusion)]

- RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation [2023-EMNLP] [[📄 paper](https://aclanthology.org/2023.emnlp-main.151/)] [[🔗 repo](https://github.com/microsoft/CodeT/tree/main/RepoCoder)]

- Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context [2023-NeurIPS] [[📄 paper](https://neurips.cc/virtual/2023/poster/70362)] [[🔗 repo](https://aka.ms/monitors4codegen)]

- CodePlan: Repository-Level Coding using LLMs and Planning [2024-FSE] [[📄 paper](https://dl.acm.org/doi/abs/10.1145/3643757)] [[🔗 repo](https://github.com/microsoft/codeplan)]

- Repoformer: Selective Retrieval for Repository-Level Code Completion [2024-ICML] [[📄 paper](https://arxiv.org/abs/2403.10059)] [[🔗 repo](https://repoformer.github.io/)]

- Iterative Refinement of Project-Level Code Context for Precise Code Generation with Compiler Feedback [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2403.16792)] [[🔗 repo](https://github.com/CGCL-codes/naturalcc/tree/main/examples/cocogen)]

- Natural Language to Class-level Code Generation by Iterative Tool-augmented Reasoning over Repository [2024-ICML] [[📄 paper](https://arxiv.org/abs/2405.01573)] [[🔗 repo](https://github.com/microsoft/repoclassbench)]

- R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2406.01359)]

- Enhancing Repository-Level Code Generation with Integrated
Contextual Information [2024-arXiv] [[📄 paper](https://arxiv.org/pdf/2406.03283)]

- STALL+: Boosting LLM-based Repository-level Code Completion with Static Analysis [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2406.10018)]

- Hierarchical Context Pruning: Optimizing Real-World Code Completion with Repository-Level Pretrained Code LLMs [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2406.18294)] [[🔗 repo](https://github.com/Hambaobao/HCP-Coder)]

- RepoMinCoder: Improving Repository-Level Code Generation Based on Information Loss Screening [2024-Internetware] [[📄 paper](https://dl.acm.org/doi/10.1145/3671016.3674819)]

- RLCoder: Reinforcement Learning for Repository-Level Code Completion [2025-ICSE] [[📄 paper](https://arxiv.org/abs/2407.19487)] [[🔗 repo](https://github.com/DeepSoftwareAnalytics/RLCoder)]

- RepoHyper: Search-Expand-Refine on Semantic Graphs for Repository-Level Code Completion [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2403.06095)] [[🔗 repo](https://github.com/FSoft-AI4Code/RepoHyper)]

- GraphCoder: Enhancing Repository-Level Code Completion via Code Context Graph-based Retrieval and Language Model [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2406.07003)]

- RepoGenReflex: Enhancing Repository-Level Code Completion with Verbal Reinforcement and Retrieval-Augmented Generation [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2409.13122)]

- RAMBO: Enhancing RAG-based Repository-Level Method Body Completion [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2409.15204)] [[🔗 repo](https://github.com/ise-uet-vnu/rambo)]

- ContextModule: Improving Code Completion via Repository-level Contextual Information [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2412.08063)]

## 📊 Datasets and Benchmarks

- **REPOCOD**: Can Language Models Replace Programmers? REPOCOD  Says 'Not Yet' [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2410.21647)]  https://github.com/lt-asset/REPOCOD

- **RepoExec**: On the Impacts of Contexts on Repository-Level Code Generation [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2406.11927)]  https://github.com/FSoft-AI4Code/RepoExec

- **R2C2-Bench**: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2406.01359)]

- **DevEval**: Evaluating Code Generation in Practical Software Projects [2024-ACL-Findings] [[📄 paper](https://aclanthology.org/2024.findings-acl.214.pdf)] [[🔗 repo](https://github.com/seketeam/DevEval)]

- **CodAgentBench**: Enhancing Code Generation with Tool-Integrated Agent Systems for Real-World Repo-level Coding Challenges [2024-ACL] [[📄 paper](https://aclanthology.org/2024.acl-long.737/)]

- **R2E-Eval**: Turning Any GitHub Repository into a Programming Agent Test Environment [2024-ICML] [[📄 paper](https://proceedings.mlr.press/v235/jain24c.html)] [[🔗 repo](https://r2e.dev/)]  

- **SWE-bench**: Can Language Models Resolve Real-World GitHub Issues? [2024-ICLR] [[📄 paper](https://arxiv.org/pdf/2310.06770)] [[🔗 repo](https://github.com/princeton-nlp/SWE-bench)]

- **RepoEval**: Repository-Level Code Completion Through Iterative Retrieval and Generation [2023-EMNLP] [[📄 paper](https://aclanthology.org/2023.emnlp-main.151/)] [[🔗 repo](https://github.com/microsoft/CodeT/tree/main/RepoCoder)]

- **CrossCodeEval**: A Diverse and Multilingual Benchmark for Cross-File Code Completion [2023-NeurIPS] [[📄 paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/920f2dced7d32ab2ba2f1970bc306af6-Paper-Datasets_and_Benchmarks.pdf)] [[🔗 site](https://crosscodeeval.github.io/)]

- **CrossCodeLongEval**: Repoformer: Selective Retrieval for Repository-Level Code Completion [2024-ICML] [[📄 paper](https://arxiv.org/abs/2403.10059)] [[🔗 repo](https://repoformer.github.io/)]

- **M2RC-EVAL**: M2rc-Eval: Massively Multilingual Repository-level Code Completion Evaluation [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2410.21157)] [[🔗 repo](https://github.com/M2RC-Eval-Team/M2RC-Eval)]

- **ExecRepoBench**: Multi-level Executable Code Completion Evaluation [2024-arXiv] [[📄 paper](https://arxiv.org/abs/2412.11990)] [[🔗 site](https://execrepobench.github.io/)]